{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eb47fc",
   "metadata": {},
   "source": [
    "### Introduction to PyArrow\n",
    "\n",
    "*   PyArrow serves as a cross-language development environment specifically designed for in-memory data.\n",
    "*   Its primary goal is to boost the performance of analytics applications.\n",
    "*   Emerging from the Apache Arrow project, PyArrow aims to make data interoperability better across different languages and systems.\n",
    "*   It uses an in-memory columnar data representation, offering an optimized memory footprint for complex data structures.\n",
    "*   With zero-copy reads, it facilitates quick data sharing between Python and other languages, sidestepping the need for serialization.\n",
    "*   It supports schemas and metadata, providing data structures that are rich and self-describing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3013192",
   "metadata": {},
   "source": [
    "### PyArrow and Parquet\n",
    "\n",
    "*   PyArrow offers seamless reading and writing operations for Parquet files.\n",
    "*   With column pruning, you can selectively read only the necessary columns from a Parquet file, reducing I/O time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b332",
   "metadata": {},
   "source": [
    "### Apache Arrow\n",
    "\n",
    "```The core feature of Apache Arrow is its in-memory columnar format. This language-agnostic standard is designed to store structured, table-like datasets efficiently in memory. The data format supports a rich set of data types, including nested and user-defined types, making it suitable for analytic databases, data frame libraries, and more.``` \n",
    "\n",
    "The Apache Arrow Project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcdbec",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/img/with_arrow.jpg\" width=700>\n",
    "</div>\n",
    "\n",
    "[picture source](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da3f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac5bc2",
   "metadata": {},
   "source": [
    "### PyArrow Data Structures\n",
    "\n",
    "*   PyArrow offers a suite of low-level data structures and methods optimized for both speed and flexibility.\n",
    "*   These structures can be used seamlessly across multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57327800",
   "metadata": {},
   "source": [
    "### Arrow Array\n",
    "\n",
    "*   An Arrow Array is essentially a column of data stored in an efficient, contiguous block of memory.\n",
    "*   Unlike Python lists, these arrays are optimized for high-speed operations and can be transferred across languages without incurring serialization costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22df9c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyarrow.lib.Int64Array'>\n",
      "---------\n",
      "[\n",
      "  1,\n",
      "  2,\n",
      "  3,\n",
      "  4,\n",
      "  5\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "arrow_array = pa.array([1, 2, 3, 4, 5])\n",
    "print(type(arrow_array))\n",
    "print(\"---------\")\n",
    "print(arrow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcda45",
   "metadata": {},
   "source": [
    "### Arrow Buffer\n",
    "\n",
    "* While not a data structure per se, Arrow Buffers are pivotal in understanding Arrow functionality.\n",
    "* Buffers are blocks of memory that house the data for Arrow Arrays, contributing to efficient storage.\n",
    "* You can even access the buffer's content directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35754c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow.Buffer address=0x36eb40300c0 size=40 is_cpu=True is_mutable=True>\n"
     ]
    }
   ],
   "source": [
    "buffer = arrow_array.buffers()[1]\n",
    "print(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c81af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "byte_data = buffer.to_pybytes()\n",
    "print(byte_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed66a0",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Here, the buffer's data contains 40 bytes, each 8 bytes representing an `int64` value for each of the 5 elements in the array.\n",
    "* You can use this buffer data to create a new NumPy array, showing that Arrow and NumPy can share memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d462d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "numpy_array = np.frombuffer(buffer, dtype=np.int64)\n",
    "numpy_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983cb2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shares_memory(arrow_array, numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac880c9",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Both `arrow_array` and `numpy_array` share the same underlying data, demonstrating the concept of zero-copy.\n",
    "* You can confirm this by modifying a value in one array and seeing the change in the other.\n",
    "  * Both arrays will now show the updated value.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6df885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array[1] = 0\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78887976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int64Array object at 0x7f8cd1812b20>\n",
       "[\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  5\n",
       "]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b45749",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "* A schema in PyArrow defines the structure, column names, and types for Arrow Arrays.\n",
    "* Schemas are crucial as they set the framework for data manipulation and operations in Arrow.\n",
    "  * Give Arrow an idea on how to encode the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186b4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column1: int64\n",
      "column2: string\n"
     ]
    }
   ],
   "source": [
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd46394",
   "metadata": {},
   "source": [
    "### Chunked Array\n",
    "\n",
    "*   A Chunked Array in PyArrow is like a single Arrow Array but divided into smaller \"chunks.\"\n",
    "*   This structure allows for the storage and processing of datasets that are too large to fit in memory.\n",
    "*   It's commonly used in distributed computing frameworks and data streaming scenarios.\n",
    "\n",
    "* For example:\n",
    "  * you could have data sent in chunks to optimize throughput\n",
    "  * you might have multiple nodes in a distributed system each producing Arrow Arrays that are collected and represented as a ChunkedArray by the master node.\n",
    "\n",
    "* From a user perspective, a Chunked Array appears as a contiguous sequence of data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c3087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x7f8cd18329f0>\n",
       "[\n",
       "  [\n",
       "    0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_node_1 = pa.array([0,1,2,3,4])\n",
    "results_node_2 = pa.array([5,6,7,8,9,10])\n",
    "chunked_array = pa.chunked_array([results_node_1, results_node_2])\n",
    "chunked_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be520fe2",
   "metadata": {},
   "source": [
    "### Chunked Array - Cont'd\n",
    "\n",
    "* You can index into a single position or even across multiple chunks, making the data handling more versatile.\n",
    "* You can also access individual chunks, allowing for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d579f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x7f8cd18431d0>\n",
       "[\n",
       "  [\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    5\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_array[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356cedc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int64Array object at 0x7f8cd1848520>\n",
       "[\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4\n",
       "]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_array.chunk(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948c663",
   "metadata": {},
   "source": [
    "### Table\n",
    "\n",
    "* A Table in PyArrow is a container for multiple Arrow ChunkedArrays with a common schema.\n",
    "* Each column in the Table is an Arrow ChunkedArray, and all columns share the same length.\n",
    "* Tables offer an ideal format for handling data in the form of a dataframe.\n",
    "* Tables can also be partitioned across multiple files for large-scale storage, or to be sent across a network, or even to be stored in-memory on a single machine.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dc32df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "column1: int64\n",
       "column2: string\n",
       "----\n",
       "column1: [[0,1,2,3,4]]\n",
       "column2: [[\"a\",\"b\",\"c\",\"d\",\"e\"]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column1 = pa.array([0, 1, 2, 3, 4]) \n",
    "column2 = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "table = pa.table({'column1': column1, 'column2': column2})  \n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaa066",
   "metadata": {},
   "source": [
    "### Record Batch\n",
    "\n",
    "*   A Record Batch is a collection of Arrow Arrays (columns) with the same length, all of which are bundled together with a schema.\n",
    "*   Much like a Chunked Array is a collection of Arrow Arrays, a Table in Apache Arrow is a collection of Record Batches.\n",
    "\n",
    "* Conceptual Relationship\n",
    "  *   In Apache Arrow, the concept of a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n",
    "    *   Arrays can be grouped together to form a Chunked Array.\n",
    "    *   Record Batches can be grouped together to form a Table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7454b",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* Use Cases\n",
    "  *   The choice between using a Record Batch or a Table often depends on your specific needs. E.g.:\n",
    "    \n",
    "  *  Streaming Data: If you need to process data on-the-fly, perhaps in a streaming application where you want to process each chunk as it arrives, Record Batches are a good choice.\n",
    "    *   You can serialize and process each Record Batch independently as they arrive, without having to wait for the entire data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5606ad83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.RecordBatch\n",
       "column1: int64\n",
       "column2: string\n",
       "----\n",
       "column1: [1,2,3,4,5]\n",
       "column2: [\"a\",\"b\",\"c\",\"d\",\"e\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column1_array = pa.array([1, 2, 3, 4, 5])\n",
    "column2_array = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "\n",
    "record_batch = pa.record_batch([column1_array, column2_array], schema=schema)\n",
    "record_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18702115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyarrow.lib.Int64Array object at 0x7f8cd1848b80>\n",
       " [\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5\n",
       " ],\n",
       " <pyarrow.lib.StringArray object at 0x7f8cd1848fa0>\n",
       " [\n",
       "   \"a\",\n",
       "   \"b\",\n",
       "   \"c\",\n",
       "   \"d\",\n",
       "   \"e\"\n",
       " ]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fe3e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int64Array object at 0x7f8cd1848be0>\n",
       "[\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5\n",
       "]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch[\"column1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359f94f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "column1: int64\n",
       "column2: string\n",
       "----\n",
       "column1: [[1,2,3,4,5],[6,7,8,9,10]]\n",
       "column2: [[\"a\",\"b\",\"c\",\"d\",\"e\"],[\"f\",\"g\",\"h\",\"i\",\"j\"]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column1_array_new = pa.array([6, 7, 8, 9, 10])\n",
    "column2_array_new = pa.array(['f', 'g', 'h', 'i', 'j'])\n",
    "record_batch_new = pa.record_batch([column1_array_new, column2_array_new], schema=schema)\n",
    "\n",
    "\n",
    "table = pa.Table.from_batches([record_batch, record_batch_new], schema=schema)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06e41",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* In the example above, two Record Batches are combined to create a single Table. \n",
    "  * This is analogous to how individual Arrow Arrays can be combined to create a Chunked Array\n",
    "  * Reinforces the idea that a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8f6b",
   "metadata": {},
   "source": [
    "### Dive Into Real Data: Parquet and Memory Efficiency\n",
    "\n",
    "1.  Let's get hands-on and read a Parquet file using Apache Arrow.\n",
    "2.  Take note: the size of the data when using PyArrow is substantially smaller than a Pandas DataFrame for the same data.\n",
    "3.  Think of this as a little teaser to whet your appetite for data science goodness.\n",
    "\n",
    "**Note**: Here, I'm using the `parquet` module from the PyArrow package. This module knows how to read Parquet files among other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4201f",
   "metadata": {},
   "source": [
    "\n",
    "### Apache Arrow Datasets\n",
    "\n",
    "\n",
    "*   Datasets in PyArrow let you work with large tabular data, even when it's larger than your machine's memory\n",
    "*   It offers lazy data access, meaning you don't have to load the entire dataset into memory.\n",
    "*   Datasets support data discovery, partitioning, and compatibility with various file systems like AWS, Google Cloud, and local storage.\n",
    "  * I can read from AWS or Google without having to install anything.\n",
    "\n",
    "* import the dataset library as:\n",
    "\n",
    "```python\n",
    "import pyarrow.dataset as ds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50e765",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "* Provider: New York City Taxi and Limousine Commission (TLC)\n",
    "* Data hosted on AWS. The URSA-LAB company account.\n",
    "* Contains data on millions of taxi and limousine trips in NYC\n",
    "* Time Period: 2009 to 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b42c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 2009/\r\n",
      "                           PRE 2010/\r\n",
      "                           PRE 2011/\r\n",
      "                           PRE 2012/\r\n",
      "                           PRE 2013/\r\n",
      "                           PRE 2014/\r\n",
      "                           PRE 2015/\r\n",
      "                           PRE 2016/\r\n",
      "                           PRE 2017/\r\n",
      "                           PRE 2018/\r\n",
      "                           PRE 2019/\r\n"
     ]
    }
   ],
   "source": [
    "# **Note**: In the AWS S3 listing, \"PRE\" stands for \"prefix,\" essentially representing a folder or directory.\n",
    "\n",
    "!aws s3 ls \"s3://ursa-labs-taxi-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4c3ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 01/\r\n",
      "                           PRE 02/\r\n",
      "                           PRE 03/\r\n",
      "                           PRE 04/\r\n",
      "                           PRE 05/\r\n",
      "                           PRE 06/\r\n",
      "                           PRE 07/\r\n",
      "                           PRE 08/\r\n",
      "                           PRE 09/\r\n",
      "                           PRE 10/\r\n",
      "                           PRE 11/\r\n",
      "                           PRE 12/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls \"s3://ursa-labs-taxi-data/2009/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3da9fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._dataset.FileSystemDataset at 0x7f8cc0c77220>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "dataset = ds.dataset(\"s3://ursa-labs-taxi-data/\", partitioning=[\"year\", \"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f60be24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdea9bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ursa-labs-taxi-data/2009/01/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/02/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/03/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/04/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/05/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/06/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/07/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/08/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/09/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/10/data.parquet']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7532427",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f432baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.count?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14a28e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07d2e4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.compute.Expression ((year == 2009) and (month == 1))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how to load just one file (a fragment) and its schema:\n",
    "\n",
    "frag = next(dataset.get_fragments())\n",
    "frag.partition_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d98407",
   "metadata": {},
   "source": [
    "#### Play with a Single File\n",
    "\n",
    "* Let's read in the data from this single fragment\n",
    "* Take a look at the data\n",
    "* List of column names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4124c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frag_table = frag.to_table()\n",
    "frag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.num_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626148d",
   "metadata": {},
   "source": [
    "#### Chunks: The Building Blocks\n",
    "\n",
    "* Remember how we talked about Arrow tables having columns that could be split into chunks? \n",
    "* If you take a look, each column is divided into 216 chunks\n",
    "  * Proving that this table is built in the way we discussed earlier.\n",
    "* Take just a slice of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[frag_table[col_name].num_chunks for col_name in frag_table.column_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774ba90",
   "metadata": {},
   "source": [
    "### The Essentials of Apache Arrow Tables and Record Batches\n",
    "\n",
    "*   Discussing how tables in Apache Arrow are essentially collections of record batches.\n",
    "*   You can easily pull data from columns like `payment_type`, `fare_amount`, or `tip_amount`. \n",
    "* Because we're working with a single record batch, managing the data is pretty straightforward. \n",
    "  * We'll see that each column, for instance, holds 65,536 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3 = frag_table.to_batches()[3]\n",
    "record_batch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3[\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3['tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3['payment_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacbb437",
   "metadata": {},
   "source": [
    "#### PyArrow's Computational Capabilities\n",
    "\n",
    "*   PyArrow separates data storage concerns from computational functionality.    \n",
    "    *   Structures like Arrow Arrays, Record Batches, and Tables handle data storage and serialization.\n",
    "    *   For actual data operations, there's the `pyarrow.compute` module.\n",
    "*   The `pyarrow.compute` module offers a range of functions for filtering, transforming, and aggregating data.    \n",
    "    *   While it does provide basic operations, it's not a full-blown analytical tool. For more complex tasks, you'd typically use something like Pandas or Spark.\n",
    "\n",
    "* Let's perform some computations like calculating the sum of tips and fares, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365b652",
   "metadata": {},
   "source": [
    "* How about finding the maximum total amount for a trip, including the tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.max(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0bcf",
   "metadata": {},
   "source": [
    "* And the average?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.mean(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a8057",
   "metadata": {},
   "source": [
    "* We can also perform operations on string data, like converting the case of `payment_type`, which has been recorded inconsistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cased_payment_type = pc.utf8_upper(record_batch_3[\"payment_type\"])\n",
    "upper_cased_payment_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3597a",
   "metadata": {},
   "source": [
    "* You can then filter data based on whether the payment type was \"CASH.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c334143",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cash = pc.equal(upper_cased_payment_type, pa.scalar('CASH'))\n",
    "is_cash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff03b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_record_batch_3 = pc.filter(record_batch_3, is_cash)\n",
    "filtered_record_batch_3\n",
    "filtered_record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7912d0",
   "metadata": {},
   "source": [
    "\n",
    "#### Working with Parquet Files\n",
    "\n",
    "*   You can read Parquet data into PyArrow as a ParquetDataset, and then work with it as ParquetFile Fragments.\n",
    "* Recall that: \n",
    "    * Each fragment has its own metadata, \n",
    "    * You can also get statistics about each row group within the fragment.\n",
    "      * However, it's usually more efficient to work with sorted data if you carry out frequent operations\n",
    "      * You can then save this sorted table into a new Parquet file for optimized data retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "dataset = pq.ParquetDataset('s3://ursa-labs-taxi-data/2009/', partitioning=[\"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13072afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = dataset.fragments[0].to_table() \n",
    "sorted_indices = pc.sort_indices(data_table, sort_keys=[(\"pickup_at\", \"ascending\"), (\"fare_amount\", \"ascending\")])\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table = data_table.take(sorted_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ee6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pq.write_table(sorted_table, 'optimized_parquet_file.parquet', row_group_size=65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33953a7",
   "metadata": {},
   "source": [
    "\n",
    "#### Exploring Sorted Parquet Files\n",
    "\n",
    "*   When you read the sorted table back into PyArrow, it's easier to work with.\n",
    "  * We can reach the read groups meta data and only look at those we are interested in.\n",
    "  * i.e., you can delve into the metadata to understand your data better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836dc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parquet_file = pq.ParquetFile('optimized_parquet_file.parquet')\n",
    "\n",
    "rg0_metadata = optimized_parquet_file.metadata.row_group(0)\n",
    "rg0_metadata.to_dict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f523dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,x[\"path_in_schema\"]) for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8972c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_2_pos = {x[\"path_in_schema\"]:i for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])}\n",
    "name_2_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "col_idx = name_2_pos['pickup_at']\n",
    "\n",
    "datetime_obj = datetime.strptime(\"2009-1-1 14:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for i in range(optimized_parquet_file.num_row_groups):\n",
    "    col_stats = optimized_parquet_file.metadata.row_group(i).column(col_idx).statistics\n",
    "    if col_stats.min <= datetime_obj <= col_stats.max:\n",
    "        print(f\"found it, it's row_group {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cf3c7",
   "metadata": {},
   "source": [
    "### Bonus Questions\n",
    "* can you get the average transaction between 2:00-2:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082403c",
   "metadata": {},
   "source": [
    "* Which day, on average has the highest tip? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb825160",
   "metadata": {},
   "source": [
    "* Which time of the day has the highest tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d501b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d45c6c",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1.  [Apache Arrow Homepage](https://arrow.apache.org/)\n",
    "2.  [PyArrow Documentation](https://arrow.apache.org/docs/python/)\n",
    "3.  [PyArrow GitHub Repository](https://github.com/apache/arrow/tree/master/python/pyarrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
